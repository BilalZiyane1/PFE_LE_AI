{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e196f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from urllib.parse import quote\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Enhanced job configuration with aliases\n",
    "JOB_CATEGORIES = [\n",
    "    {\n",
    "        \"category\": \"commerce & teleconseille\",\n",
    "        \"aliases\": [\n",
    "            \"commerce & teleconseille\",\n",
    "            \"sales and customer service\",\n",
    "            \"téléconseiller\",\n",
    "            \"customer support\",\n",
    "            \"telemarketing\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"maintenance informatique\",\n",
    "        \"aliases\": [\n",
    "            \"maintenance informatique\",\n",
    "            \"IT maintenance\",\n",
    "            \"computer maintenance\",\n",
    "            \"IT support\",\n",
    "            \"systèmes informatiques\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"community management\",\n",
    "        \"aliases\": [\n",
    "            \"community management\",\n",
    "            \"social media management\",\n",
    "            \"gestion de communauté\",\n",
    "            \"online community manager\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"frontend developement\",\n",
    "        \"aliases\": [\n",
    "            \"frontend developement\",  # Keeping original typo\n",
    "            \"frontend development\",\n",
    "            \"développement frontend\",\n",
    "            \"web development\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Creation du jeu\",\n",
    "        \"aliases\": [\n",
    "            \"Creation du jeu\",\n",
    "            \"game development\",\n",
    "            \"développement de jeu\",\n",
    "            \"video game design\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"marketing digital\",\n",
    "        \"aliases\": [\n",
    "            \"marketing digital\",\n",
    "            \"digital marketing\",\n",
    "            \"e-marketing\",\n",
    "            \"web marketing\"\n",
    "        ]\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"Creation du contenu\",\n",
    "        \"aliases\": [\n",
    "            \"Creation du contenu\",\n",
    "            \"content creation\",\n",
    "            \"création de contenu\",\n",
    "            \"content marketing\"\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n",
    "LOCATIONS = [\n",
    "    \"Morocco\",\n",
    "    \"Europe\",\n",
    "    \"Middle East\",\n",
    "    \"USA\",\n",
    "    \"Canada\"\n",
    "]\n",
    "\n",
    "MAX_RESULTS_PER_QUERY = 1000\n",
    "RESULTS_PER_PAGE = 25\n",
    "MAX_THREADS = 5\n",
    "DELAY_RANGE = (1, 3)  # Seconds between requests\n",
    "USER_AGENTS = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "]\n",
    "\n",
    "def get_random_headers():\n",
    "    return {\n",
    "        \"User-Agent\": random.choice(USER_AGENTS),\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\",\n",
    "        \"Referer\": \"https://www.google.com/\"\n",
    "    }\n",
    "\n",
    "def scrape_job_listings(search_term, original_category, location, seen_job_ids):\n",
    "    job_listings = []\n",
    "    encoded_job = quote(search_term)\n",
    "    encoded_location = quote(location)\n",
    "    \n",
    "    for start in range(0, MAX_RESULTS_PER_QUERY, RESULTS_PER_PAGE):\n",
    "        time.sleep(random.uniform(*DELAY_RANGE))\n",
    "        list_url = f\"https://www.linkedin.com/jobs-guest/jobs/api/seeMoreJobPostings/{encoded_job}-jobs?keywords={encoded_job}&location={encoded_location}&start={start}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(list_url, headers=get_random_headers())\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "                print(f\"Failed to fetch {search_term} in {location} at start={start}\")\n",
    "                break\n",
    "                \n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            jobs = soup.find_all(\"li\")\n",
    "            \n",
    "            if not jobs:\n",
    "                break\n",
    "                \n",
    "            for job in jobs:\n",
    "                base_card_div = job.find(\"div\", {\"class\": \"base-card\"})\n",
    "                if not base_card_div:\n",
    "                    continue\n",
    "                    \n",
    "                job_id = base_card_div.get(\"data-entity-urn\", \"\").split(\":\")[-1]\n",
    "                if not job_id or job_id in seen_job_ids:\n",
    "                    continue\n",
    "                    \n",
    "                seen_job_ids.add(job_id)\n",
    "                job_listings.append({\n",
    "                    \"job_id\": job_id,\n",
    "                    \"original_category\": original_category,\n",
    "                    \"search_location\": location,\n",
    "                    \"search_term_used\": search_term\n",
    "                })\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error scraping {search_term} in {location}: {str(e)}\")\n",
    "            continue\n",
    "            \n",
    "    return job_listings\n",
    "\n",
    "def scrape_job_details(job):\n",
    "    job_url = f\"https://www.linkedin.com/jobs/view/{job['job_id']}\"\n",
    "    job_post = {\n",
    "        \"job_id\": job[\"job_id\"],\n",
    "        \"original_category\": job[\"original_category\"],\n",
    "        \"search_location\": job[\"search_location\"],\n",
    "        \"search_term_used\": job[\"search_term_used\"],\n",
    "        \"job_title\": None,\n",
    "        \"company_name\": None,\n",
    "        \"company_url\": None,\n",
    "        \"location\": None,\n",
    "        \"time_posted\": None,\n",
    "        \"num_applicants\": None,\n",
    "        \"employment_type\": None,\n",
    "        \"job_level\": None,\n",
    "        \"job_description\": None,\n",
    "        \"job_url\": job_url\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        time.sleep(random.uniform(*DELAY_RANGE))\n",
    "        response = requests.get(job_url, headers=get_random_headers())\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "            \n",
    "            # Extract job title\n",
    "            title_element = soup.find(\"h1\", {\"class\": \"top-card-layout__title\"})\n",
    "            if title_element:\n",
    "                job_post[\"job_title\"] = title_element.get_text(strip=True)\n",
    "            \n",
    "            # Extract company info\n",
    "            company_element = soup.find(\"a\", {\"class\": \"topcard__org-name-link\"})\n",
    "            if company_element:\n",
    "                job_post[\"company_name\"] = company_element.get_text(strip=True)\n",
    "                job_post[\"company_url\"] = company_element.get(\"href\", \"\")\n",
    "            \n",
    "            # Extract location\n",
    "            location_element = soup.find(\"span\", {\"class\": \"topcard__flavor--bullet\"})\n",
    "            if location_element:\n",
    "                job_post[\"location\"] = location_element.get_text(strip=True)\n",
    "            \n",
    "            # Extract posting time\n",
    "            time_element = soup.find(\"span\", {\"class\": \"posted-time-ago__text\"})\n",
    "            if time_element:\n",
    "                job_post[\"time_posted\"] = time_element.get_text(strip=True)\n",
    "            \n",
    "            # Extract applicants\n",
    "            applicants_element = soup.find(\"span\", {\"class\": \"num-applicants__caption\"})\n",
    "            if applicants_element:\n",
    "                job_post[\"num_applicants\"] = applicants_element.get_text(strip=True)\n",
    "            \n",
    "            # Extract employment details\n",
    "            criteria_elements = soup.find_all(\"span\", {\"class\": \"description__job-criteria-text\"})\n",
    "            if criteria_elements:\n",
    "                job_post[\"employment_type\"] = criteria_elements[0].get_text(strip=True) if len(criteria_elements) > 0 else None\n",
    "                job_post[\"job_level\"] = criteria_elements[1].get_text(strip=True) if len(criteria_elements) > 1 else None\n",
    "            \n",
    "            # Extract job description\n",
    "            description_element = soup.find(\"div\", {\"class\": \"show-more-less-html__markup\"})\n",
    "            if description_element:\n",
    "                job_post[\"job_description\"] = description_element.get_text(strip=True)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping job {job['job_id']}: {str(e)}\")\n",
    "    \n",
    "    return job_post\n",
    "\n",
    "def main():\n",
    "    seen_job_ids = set()\n",
    "    all_jobs = []\n",
    "    \n",
    "    # Phase 1: Collect job listings with aliases\n",
    "    for job_category in JOB_CATEGORIES:\n",
    "        original_category = job_category[\"category\"]\n",
    "        print(f\"\\nProcessing category: {original_category}\")\n",
    "        \n",
    "        for alias in job_category[\"aliases\"]:\n",
    "            for location in LOCATIONS:\n",
    "                print(f\"Searching: '{alias}' in {location}\")\n",
    "                listings = scrape_job_listings(alias, original_category, location, seen_job_ids)\n",
    "                all_jobs.extend(listings)\n",
    "                print(f\"Found {len(listings)} new listings using '{alias}'\")\n",
    "\n",
    "    print(f\"\\nTotal unique jobs found: {len(all_jobs)}\")\n",
    "    \n",
    "    # Phase 2: Scrape detailed information\n",
    "    job_details = []\n",
    "    with ThreadPoolExecutor(max_workers=MAX_THREADS) as executor:\n",
    "        futures = [executor.submit(scrape_job_details, job) for job in all_jobs]\n",
    "        for i, future in enumerate(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                job_details.append(result)\n",
    "                if (i + 1) % 10 == 0:\n",
    "                    print(f\"Processed {i+1}/{len(all_jobs)} jobs\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing job: {str(e)}\")\n",
    "    \n",
    "    # Save results\n",
    "    df = pd.DataFrame(job_details)\n",
    "    df.to_csv(\"linkedin_jobs_enhanced.csv\", index=False)\n",
    "    print(\"\\nScraping complete. Data saved to linkedin_jobs_enhanced.csv\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
